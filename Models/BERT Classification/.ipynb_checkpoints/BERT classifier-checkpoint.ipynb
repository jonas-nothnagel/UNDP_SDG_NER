{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fcfbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvida-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvida-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc004b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4462a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef24892",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'whitegrid', palette = 'muted', font_scale = 1.3)\n",
    "rcParams['figure.figsize'] = 13, 7\n",
    "\n",
    "random_seed = 777\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6918c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('17SDGpt2_tweets.csv', lineterminator='\\n', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249fa307",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df.sdg)\n",
    "plt.xlabel('sdg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained Model \n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d96539",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d84cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = 'Sustained and inclusive economic growth can drive progress, create decent jobs for all and improve living standards.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sentence : {sample_txt}\")\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Token IDs : {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c80f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(sample_txt,\n",
    "                                max_length = 32,\n",
    "                                add_special_tokens = True,\n",
    "                                truncation = True,\n",
    "                                return_token_type_ids = False,\n",
    "                                padding = True,\n",
    "                                return_attention_mask = True,\n",
    "                                return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcf62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40338dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encoding['input_ids'][0]))\n",
    "encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Choosing a sequence Length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(df):\n",
    "    df = df.copy().reset_index(drop = True)\n",
    "    df = df.apply(lambda x: re.sub(r\"http\\S+\", \"\", x), 1)\\\n",
    ".apply(lambda i: \" \".join(filter(lambda x:x[0]!=\"@\", i.split())), 1)\\\n",
    ".apply(lambda x: re.sub(r\"&amp\", \"\",x),1)\\\n",
    ".apply(lambda x: re.sub(r\"&amp;\", \"\",x),1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5455823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = clean_text(df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['clean_tweet'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['clean_tweet'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f516cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['sdg', 'clean_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "for tweet in df['clean_tweet']:\n",
    "    tokens = tokenizer.encode(tweet, max_length = 200, truncation = True)\n",
    "    token_lens.append(len(tokens))\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0,250])\n",
    "plt.xlabel(['Token Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85616ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sdgdataset(Dataset):\n",
    "    def __init__(self, tweets, sdg, tokenizer, max_len):\n",
    "        self.tweets = tweets\n",
    "        self.sdg = sdg\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        tweets = str(self.tweets[item])\n",
    "        sdg = self.sdg[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "        tweets,\n",
    "        add_special_tokens = True,\n",
    "        max_length = self.max_len,\n",
    "        return_token_type_ids = False,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt')\n",
    "        \n",
    "        return {'tweet_text': tweet,\n",
    "               'input_ids': encoding['input_ids'].flatten(),\n",
    "               'attention_mask': encoding['attention_mask'].flatten(),\n",
    "               'sdg': torch.tensor(sdg, dtype = torch.long)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.4,\n",
    "                                     random_state = random_seed)\n",
    "\n",
    "df_val, df_test = train_test_split(df_test, test_size = 0.5,\n",
    "                                   random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae00964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = sdgdataset(tweets = df.clean_tweet.to_numpy(),\n",
    "                        sdg = df.sdg.to_numpy(),\n",
    "                        tokenizer = tokenizer,\n",
    "                        max_len = max_len)\n",
    "    \n",
    "    return DataLoader(ds,\n",
    "                     batch_size = batch_size,\n",
    "                     num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d68e253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b50cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c98d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38128fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a679c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
