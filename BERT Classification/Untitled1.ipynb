{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e603a53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catalyst.dl.callbacks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-02010ffb8d51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Catalyst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcatalyst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSupervisedRunner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcatalyst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAccuracyCallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF1ScoreCallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptimizerCallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcatalyst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCheckpointCallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInferCallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcatalyst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_global_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepare_cudnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catalyst.dl.callbacks'"
     ]
    }
   ],
   "source": [
    "# Python \n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Mapping, List\n",
    "from pprint import pprint\n",
    "\n",
    "# Numpy and Pandas \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers \n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# Catalyst\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.dl.callbacks import AccuracyCallback, F1ScoreCallback, OptimizerCallback\n",
    "from catalyst.dl.callbacks import CheckpointCallback, InferCallback\n",
    "from catalyst.utils import set_global_seed, prepare_cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee53efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased' # pretrained model from Transformers\n",
    "NUM_EPOCHS = 3                         # smth around 2-6 epochs is typically fine when finetuning transformers\n",
    "BATCH_SIZE = 72                        # depends on your available GPU memory (in combination with max seq length)\n",
    "MAX_SEQ_LENGTH = 150                   # depends on your available GPU memory (in combination with batch size)\n",
    "LEARN_RATE = 5e-5                      # learning rate is typically ~1e-5 for transformers\n",
    "ACCUM_STEPS = 4                        # one optimization step for that many backward passes\n",
    "SEED = 777                              # random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f2e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('17SDGpt2_tweets.csv', lineterminator='\\n', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0c586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(df):\n",
    "    df = df.copy().reset_index(drop = True)\n",
    "    df = df.apply(lambda x: re.sub(r\"http\\S+\", \"\", x), 1)\\\n",
    ".apply(lambda i: \" \".join(filter(lambda x:x[0]!=\"@\", i.split())), 1)\\\n",
    ".apply(lambda x: re.sub(r\"&amp\", \"\",x),1)\\\n",
    ".apply(lambda x: re.sub(r\"&amp;\", \"\",x),1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2c7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = clean_text(df['tweet'])\n",
    "df['clean_tweet'] = df['clean_tweet'].drop_duplicates()\n",
    "df = df[~df['clean_tweet'].isnull()]\n",
    "df = df[['sdg', 'clean_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d269d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.4,\n",
    "                                     random_state = SEED)\n",
    "\n",
    "df_val, df_test = train_test_split(df_test, test_size = 0.5,\n",
    "                                   random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa29c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     0.140685\n",
       "15    0.081566\n",
       "9     0.080457\n",
       "3     0.070314\n",
       "16    0.068364\n",
       "12    0.065887\n",
       "11    0.056372\n",
       "8     0.053324\n",
       "10    0.050287\n",
       "7     0.050175\n",
       "5     0.050130\n",
       "17    0.044963\n",
       "13    0.043708\n",
       "2     0.038093\n",
       "6     0.035661\n",
       "1     0.035404\n",
       "14    0.034608\n",
       "Name: sdg, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target distribution\n",
    "df_train['sdg'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd5428b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    89228.000000\n",
       "mean        31.540593\n",
       "std         11.718447\n",
       "min          1.000000\n",
       "25%         22.000000\n",
       "50%         31.000000\n",
       "75%         41.000000\n",
       "max         70.000000\n",
       "Name: clean_tweet, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statistics of text length (in words)\n",
    "df_train['clean_tweet'].apply(lambda s: len(s.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be7b755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper around Torch Dataset to perform text classification\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 texts: List[str],\n",
    "                 labels: List[str] = None,\n",
    "                 label_dict: Mapping[str, int] = None,\n",
    "                 max_seq_length: int = 512,\n",
    "                 model_name: str = 'distilbert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (List[str]): a list with texts to classify or to train the\n",
    "                classifier on\n",
    "            labels List[str]: a list with classification labels (optional)\n",
    "            label_dict (dict): a dictionary mapping class names to class ids,\n",
    "                to be passed to the validation data (optional)\n",
    "            max_seq_length (int): maximal sequence length in tokens,\n",
    "                texts will be stripped to this length\n",
    "            model_name (str): transformer model name, needed to perform\n",
    "                appropriate tokenization\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.label_dict = label_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        if self.label_dict is None and labels is not None:\n",
    "            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n",
    "            # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
    "            # no easily handle unknown target values\n",
    "            self.label_dict = dict(zip(sorted(set(labels)),\n",
    "                                       range(len(set(labels)))))\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # suppresses tokenizer warnings\n",
    "        logging.getLogger(\n",
    "            \"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
    "\n",
    "        # special tokens for transformers\n",
    "        # in the simplest case a [CLS] token is added in the beginning\n",
    "        # and [SEP] token is added in the end of a piece of text\n",
    "        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n",
    "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
    "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
    "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n",
    "        \"\"\"Gets element of the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int): index of the element in the dataset\n",
    "        Returns:\n",
    "            Single element by index\n",
    "        \"\"\"\n",
    "\n",
    "        # encoding the text\n",
    "        x = self.texts[index]\n",
    "        x_encoded = self.tokenizer.encode(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # padding short texts\n",
    "        true_seq_length = x_encoded.size(0)\n",
    "        pad_size = self.max_seq_length - true_seq_length\n",
    "        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
    "        x_tensor = torch.cat((x_encoded, pad_ids))\n",
    "\n",
    "        # dealing with attention masks - there's a 1 for each input token and\n",
    "        # if the sequence is shorter that `max_seq_length` then the rest is\n",
    "        # padded with zeroes. Attention mask will be passed to the model in\n",
    "        # order to compute attention scores only with input data\n",
    "        # ignoring padding\n",
    "        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n",
    "        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n",
    "        mask = torch.cat((mask, mask_pad))\n",
    "\n",
    "        output_dict = {\n",
    "            \"features\": x_tensor,\n",
    "            'attention_mask': mask\n",
    "        }\n",
    "\n",
    "        # encoding target\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "            y_encoded = torch.Tensor(\n",
    "                [self.label_dict.get(y, -1)]\n",
    "            ).long().squeeze(0)\n",
    "            output_dict[\"targets\"] = y_encoded\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb6a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(\n",
    "    texts=df_train['clean_tweet'].values.tolist(),\n",
    "    labels=df_train['sdg'].values.tolist(),\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "valid_dataset = TextClassificationDataset(\n",
    "    texts=df_val['clean_tweet'].values.tolist(),\n",
    "    labels=df_val['sdg'].values.tolist(),\n",
    "    label_dict=train_dataset.label_dict,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "test_dataset = TextClassificationDataset(\n",
    "    texts=df_test['clean_tweet'].values.tolist(),\n",
    "    labels=None,\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9a151d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(train_dataset.label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55ad85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loaders = {\n",
    "    \"train\": DataLoader(dataset=train_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=True),\n",
    "    \"valid\": DataLoader(dataset=valid_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False)    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a0c4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified version of the same class by HuggingFace.\n",
    "    See transformers/modeling_distilbert.py in the transformers repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model_name: str, num_classes: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pretrained_model_name (str): HuggingFace model name.\n",
    "                See transformers/modeling_auto.py\n",
    "            num_classes (int): the number of class labels\n",
    "                in the classification task\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name, num_labels=num_classes)\n",
    "\n",
    "        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n",
    "                                                    config=config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, num_classes)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "    def forward(self, features, attention_mask=None, head_mask=None):\n",
    "        \"\"\"Compute class probabilities for the input sequence.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): ids of each token,\n",
    "                size ([bs, seq_length]\n",
    "            attention_mask (torch.Tensor): binary tensor, used to select\n",
    "                tokens which are used to compute attention scores\n",
    "                in the self-attention heads, size [bs, seq_length]\n",
    "            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n",
    "                we keep the head, size: [num_heads]\n",
    "                or [num_hidden_layers x num_heads]\n",
    "        Returns:\n",
    "            PyTorch Tensor with predicted class probabilities\n",
    "        \"\"\"\n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "        distilbert_output = self.distilbert(input_ids=features,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            head_mask=head_mask)\n",
    "        # we only need the hidden state here and don't need\n",
    "        # transformer output, so index 0\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        # we take embeddings from the [CLS] token, so again index 0\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d12766fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification(pretrained_model_name=MODEL_NAME,\n",
    "                                            num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a81f50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fa7f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"    # can be changed in case of multiple GPUs onboard\n",
    "set_global_seed(SEED)                       # reproducibility\n",
    "prepare_cudnn(deterministic=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64251a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AccuracyCallback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AccuracyCallback' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# here we specify that we pass masks to the runner. So model's forward method will be called with\n",
    "# these arguments passed to it. \n",
    "runner = SupervisedRunner(\n",
    "    input_key=(\n",
    "        \"features\",\n",
    "        \"attention_mask\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=train_val_loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=NUM_CLASSES),\n",
    "#         F1ScoreCallback(activation='Softmax'), # throws a tensor shape mismatch error\n",
    "        OptimizerCallback(accumulation_steps=ACCUM_STEPS)\n",
    "    ],\n",
    "    fp16=FP16_PARAMS,\n",
    "    logdir=LOG_DIR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3dd81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
